
import json, uuid, os, torch
from typing import List, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
import openai
import re
from dotenv import load_dotenv
# from google.colab import userdata

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")


# ### DB êµ¬ì¶•í•˜ê¸°

# In[3]:


embedding_model = SentenceTransformer('dragonkue/BGE-m3-ko')


# In[ ]:

DB_PATH = "qdrant_bge"
COLLECTION_NAME = "dcinside"

# ì¸ë±ìŠ¤ ìƒì„±í•˜ê¸°
def database_indexing(client):
    data_path = "merged_dataset.json"

    with open(data_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    points_to_upsert = []
    batch_size = 100

    for idx, item in enumerate(dataset):
        main_text = item.get('main', '')
        comments = item.get('comments', [])
        comments_text = ' '.join(comments) if comments else ''

        full_text = f"{main_text} {comments_text}".strip()

        if not full_text:
            continue

        vector = embedding_model.encode(full_text).tolist()

        payload = {
            "date": item.get('date', ''),
            "main": main_text,
            "comments": comments,
            "source_url": item.get('source_url', ''),
            "gallery": item.get('gallery', ''),
            "full_text": full_text
        }

        point = PointStruct(
            id=str(uuid.uuid4()),
            vector=vector,
            payload=payload
        )
        points_to_upsert.append(point)

        if len(points_to_upsert) >= batch_size:
            client.upsert(
                collection_name=COLLECTION_NAME,
                points=points_to_upsert,
                wait=True
            )
            print(f"Indexed {idx + 1}/{len(dataset)} items...")
            points_to_upsert = []

    if points_to_upsert:
        client.upsert(
            collection_name=COLLECTION_NAME,
            points=points_to_upsert,
            wait=True
        )

    print(f"index completed : {len(dataset)}")

    return client


def database_check():
    client = QdrantClient(path=DB_PATH)

    collections = client.get_collections()
    collection_names = [c.name for c in collections.collections]

    if COLLECTION_NAME in collection_names:
        print(f" existing collection '{COLLECTION_NAME}'")
    else:
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
        )
        print(f" new collection '{COLLECTION_NAME}' ")
        client = database_indexing(client)
    return client





# In[ ]:


# openai.api_key = userdata.get('OPENAI_KEY')


# ### RAG í•¨ìˆ˜ ì •ì˜

# In[8]:


def search_documents(client, query: str, top_k: int = 3, gallery_filter: str = None) -> List[Dict]:
    query_vector = embedding_model.encode(query).tolist()

    query_filter = None
    if gallery_filter:
        from qdrant_client.models import Filter, FieldCondition, MatchValue
        query_filter = Filter(
            must=[
                FieldCondition(
                    key="gallery",
                    match=MatchValue(value=gallery_filter)
                )
            ]
        )

    search_response = client.query_points(
        collection_name=COLLECTION_NAME,
        query=query_vector,
        query_filter=query_filter,
        limit=top_k,
        with_payload=True
    )

    results = []
    for hit in search_response.points:
        results.append({
            "score": hit.score,
            "date": hit.payload.get('date', ''),
            "main": hit.payload.get('main', ''),
            "comments": hit.payload.get('comments', []),
            "source_url": hit.payload.get('source_url', ''),
            "gallery": hit.payload.get('gallery', ''),
            "full_text": hit.payload.get('full_text', '')
        })

    return results


# In[9]:


def _prepare_rag_context(client, query: str, top_k: int = 3, gallery_filter: str = None) -> tuple:
    retrieved_docs = search_documents(client, query, top_k=top_k, gallery_filter=gallery_filter)

    context_parts = []
    for i, doc in enumerate(retrieved_docs, 1):
        context_parts.append(f"[ë¬¸ì„œ {i}]")
        context_parts.append(f"ë‚ ì§œ: {doc['date']}")
        context_parts.append(f"ê°¤ëŸ¬ë¦¬: {doc['gallery']}")
        context_parts.append(f"ë‚´ìš©: {doc['main']}")
        if doc['comments']:
            context_parts.append(f"ëŒ“ê¸€: {', '.join(doc['comments'][:3])}")
        context_parts.append("")

    context = "\n".join(context_parts)
    return retrieved_docs, context


# apië¡œ ë°ë ¤ì˜¤ê¸°

# In[10]:


def generate_rag_response_api(
    query: str,
    model: str = "gpt-4o-mini",
    top_k: int = 3,
    gallery_filter: str = None,
    max_tokens: int = 512,
    temperature: float = 0.7
) -> Dict[str, Any]:

    retrieved_docs, context = _prepare_rag_context(query, top_k, gallery_filter)
    prompt = f"""ë‹¤ìŒ ë¬¸ì„œì— ìˆëŠ”ëŒ€ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""

    response = openai.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œì— ìˆëŠ”ëŒ€ë¡œ ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=max_tokens,
        temperature=temperature
    )
    answer = response.choices[0].message.content.strip()

    return {
        "query": query,
        "answer": answer,
        "retrieved_docs": retrieved_docs,
        "model_used": model
    }


# In[ ]:


# get_ipython().system('ls -lh ~/.cache/huggingface/hub | grep models')


# ë¡œì»¬ ëª¨ë¸ë¡œ ë°ë ¤ì˜¤ê¸°

# In[14]:

def model_setup(mode):
    load_dotenv()
    if mode == "base": model_name = os.getenv("BASE_MODEL_NAME")
    elif mode == "detox": model_name = os.getenv("DETOX_MODEL_NAME")
    else:
        print("wrong mode selection. Choose between 0 and 1.")
        return
    
    print(model_name)

    # ì–‘ìí™” í•„ìš”í•  ì‹œ (ì½”ë© ê¸°ì¤€ 8B ì´ìƒ)
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        # quantization_config=quantization_config, # í•„ìš”í•  ì‹œ
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        # device_map="auto" if device == "cuda" else None,
        low_cpu_mem_usage=True
    ).to(device)

    print('load done !')
    
    return tokenizer, model


# In[ ]:


def generate_rag_response_local(
    tokenizer,
    model,
    client,
    summary,
    query: str,
    top_k: int = 10,
    gallery_filter: str = None,
    max_tokens: int = 150,
    temperature: float = 0.45,
) -> Dict[str, Any]:

    retrieved_docs, _ = _prepare_rag_context(client, query, top_k, gallery_filter)

    print(f"\nğŸ” ê²€ìƒ‰ëœ ì›ë³¸ ë¬¸ì„œ ê°œìˆ˜: {len(retrieved_docs)}ê°œ")

    # 2. ì¤‘ë³µ ì œê±° ë° ë‚´ìš© ë³‘í•©
    unique_docs = []
    seen_contents = set()

    for doc in retrieved_docs:
        # ë³¸ë¬¸ê³¼ ëŒ“ê¸€ì„ ëª¨ë‘ í•©ì³ì„œ í’ë¶€í•œ Context ë§Œë“¤ê¸°
        content_parts = [doc['main'].strip()]
        if doc['comments']:
            # ëŒ“ê¸€ë„ ë‚´ìš©ì— í¬í•¨ (ë„ˆë¬´ ì§§ì€ ë³¸ë¬¸ ë³´ì™„)
            content_parts.extend(doc['comments'][:2])
        
        full_content = " ".join(content_parts)
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì¤‘ë³µì´ë©´ íŒ¨ìŠ¤, ì•„ë‹ˆë©´ ì¶”ê°€
        if len(full_content) > 2 and full_content not in seen_contents:
            unique_docs.append(doc)
            seen_contents.add(full_content)

    # [ë””ë²„ê¹…] ì¤‘ë³µ ì œê±° í›„ ê°œìˆ˜ ì¶œë ¥
    print(f"âœ¨ ì¤‘ë³µ ì œê±° í›„ ë‚¨ì€ ë¬¸ì„œ: {len(unique_docs)}ê°œ")

    # Context êµ¬ì„±
    context_list = []
    for doc in unique_docs:
        # ë³¸ë¬¸ + ëŒ“ê¸€ ê²°í•©
        text = doc['main']
        if doc['comments']:
            text += " (" + " ".join(doc['comments'][:2]) + ")"
        context_list.append(text)
        
    context = "\n".join(context_list)

    print("\n" + "="*20 + " [Context Preview] " + "="*20)
    print(context[:500] + "..." if len(context) > 500 else context)
    print("="*60 + "\n")

    # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ë¦¬í„´
    if not context.strip():
        return {
            "query": query,
            "answer": "ê´€ë ¨ëœ ë‚´ìš©ì´ DBì— ì—†ìŒ.",
            "retrieved_docs": []
        }

    print(f"\nğŸ” ê²€ìƒ‰ëœ ì›ë³¸ ë¬¸ì„œ ê°œìˆ˜: {len(retrieved_docs)}ê°œ")

    # Prompt êµ¬ì„±
    prompt = f"""
#Constraints. 
##1. Answer the question based ONLY on the provided Documents below.
##2. Do not make up information.
##3. you MUST answer in **KOREAN**.
##4. Output a string of the following format: 
    - "your answer ONLY"
- ex1) 
    "ì—¬ê°€ë¶€ê°™ì€ ì†Œë¦¬í•˜ê³  ì•‰ì•˜ë…¸ã…‹ã…‹ ì ˆëŒ€ ì•ˆëœë‹¤ ê²Œì´ì•¼ã…‹ã…‹ã…‹ã…‹ã…‹"
- ex2)
    "ëª¨ë³‘ì œëŠ” ì”¨ë°œ ì¢†ê°™ì€ ì†Œë¦¬í•˜ê³  ì•‰ì•˜ë„¤ ã„¹ã…‡"


#Documents:       
-{context}

#Question: 
-{summary}

Answer:
  """
    print("\n")
    print("====final prompt====")
    print(prompt)

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.8,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty = 1.3
        )

    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    answer = generated_text.split("Answer:")[-1].strip()

    return answer


def generate_rag_response_local_old(
    tokenizer,
    model,
    client,
    query: str,
    top_k: int = 10,
    gallery_filter: str = None,
    max_tokens: int = 150,
    temperature: float = 0.45,
) -> Dict[str, Any]:

    print("="*50)
    print('\n')
    print("query: ")
    print(query)

    retrieved_docs, _ = _prepare_rag_context(client, query, top_k, gallery_filter)

    print(f"\nğŸ” ê²€ìƒ‰ëœ ì›ë³¸ ë¬¸ì„œ ê°œìˆ˜: {len(retrieved_docs)}ê°œ")

    # 2. ì¤‘ë³µ ì œê±° ë° ë‚´ìš© ë³‘í•©
    unique_docs = []
    seen_contents = set()

    for doc in retrieved_docs:
        # ë³¸ë¬¸ê³¼ ëŒ“ê¸€ì„ ëª¨ë‘ í•©ì³ì„œ í’ë¶€í•œ Context ë§Œë“¤ê¸°
        content_parts = [doc['main'].strip()]
        if doc['comments']:
            # ëŒ“ê¸€ë„ ë‚´ìš©ì— í¬í•¨ (ë„ˆë¬´ ì§§ì€ ë³¸ë¬¸ ë³´ì™„)
            content_parts.extend(doc['comments'][:2])
        
        full_content = " ".join(content_parts)
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì¤‘ë³µì´ë©´ íŒ¨ìŠ¤, ì•„ë‹ˆë©´ ì¶”ê°€
        if len(full_content) > 2 and full_content not in seen_contents:
            unique_docs.append(doc)
            seen_contents.add(full_content)

    # [ë””ë²„ê¹…] ì¤‘ë³µ ì œê±° í›„ ê°œìˆ˜ ì¶œë ¥
    print(f"âœ¨ ì¤‘ë³µ ì œê±° í›„ ë‚¨ì€ ë¬¸ì„œ: {len(unique_docs)}ê°œ")

    # Context êµ¬ì„±
    context_list = []
    for doc in unique_docs:
        # ë³¸ë¬¸ + ëŒ“ê¸€ ê²°í•©
        text = doc['main']
        if doc['comments']:
            text += " (" + " ".join(doc['comments'][:2]) + ")"
        context_list.append(text)
        
    context = "\n".join(context_list)

    print("\n" + "="*20 + " [Context Preview] " + "="*20)
    print(context[:500] + "..." if len(context) > 500 else context)
    print("="*60 + "\n")

    # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ë°”ë¡œ ë¦¬í„´
    if not context.strip():
        return {
            "query": query,
            "answer": "ê´€ë ¨ëœ ë‚´ìš©ì´ DBì— ì—†ìŒ.",
            "retrieved_docs": []
        }
    
    # Prompt êµ¬ì„±
    messages = [
        {
            "role": "system",
            "content": (
                "ë„ˆëŠ” ë””ì‹œì¸ì‚¬ì´ë“œ ê°¤ëŸ¬ë¦¬ ìœ ì €ë‹¤. "
                "ì£¼ì–´ì§„ [í…ìŠ¤íŠ¸]ë“¤ì˜ ë‚´ìš©ì„ ì¢…í•©í•´ì„œ **ë°˜ë§(ë¹„ì†ì–´, ìŒìŠ´ì²´)**ë¡œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ë¼. "
                "ë‹ˆ ìƒê°ì€ ë„£ì§€ ë§ê³ , í…ìŠ¤íŠ¸ì— ìˆëŠ” ì—¬ë¡ ë§Œ ê·¸ëŒ€ë¡œ ìŠì–´ë¼."
            )
        },
        {
            "role": "user",
            "content": (
                f"ì•„ë˜ [í…ìŠ¤íŠ¸] ì½ê³  ë””ì‹œ ë§íˆ¬ë¡œ í•œ ì¤„ ìš”ì•½í•´.\n"
                f"ë‚´ìš©:\n{context}\n\n"
                f"ìš”ì•½:"
            )
        }
    ]

    print("="*50)
    print('\n')
    print("FINAL PROMPT: ")
    print(messages)
    


    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)

    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            max_new_tokens=max_tokens,
            eos_token_id = terminators,
            do_sample=True,
            temperature=temperature,
            top_p=0.9,
            repetition_penalty = 1.1,
            no_repeat_ngram_size = 0
        )

    response = outputs[0][input_ids.shape[-1]:]
    answer = tokenizer.decode(response, skip_special_tokens = True).strip()

    # í›„ì²˜ë¦¬
    if "ìš”ì•½" in answer:
        answer = answer.split("ìš”ì•½:")[-1].strip()
        
    # ë¶ˆí•„ìš”í•œ ê¸°í˜¸/ì˜ì–´ ì œê±° (í•œê¸€, ìˆ«ì, ë¬¸ì¥ë¶€í˜¸ë§Œ)
    answer = re.sub(r'[^ê°€-í£0-9\s.,!?~ã…‹ã…]', '', answer)
    answer = re.sub(r'\s+', ' ', answer).strip()

    return {
        "query": query,
        "answer": answer,
        "retrieved_docs": unique_docs
    }



# client = database_check()
# tokenizer, model = model_setup("base")
# answer = generate_rag_response_local_old(tokenizer, model, client, "ì—¬ì„±ê°€ì¡±ë¶€ë¥¼ í˜ì§€í•´ì•¼ í• ê¹Œ?" )
# print("response: ")
# print(answer)



# In[ ]:


def interactive_rag():
    while True:
        user_input = input("\nì§ˆë¬¸: ").strip()
        if user_input.lower() in ['quit', 'exit']:
            print("\nëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break


# In[ ]:


